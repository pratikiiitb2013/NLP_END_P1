{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ENDS13_Submission.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQR+RPwQp3kVWbRVuyoRNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikiiitb2013/NLP_END_P1/blob/main/Session13/%20ENDS13_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxiGyuCosWcI"
      },
      "source": [
        "![](https://raw.githubusercontent.com/bentrevett/pytorch-seq2seq/9479fcb532214ad26fd4bda9fcf081a05e1aaf4e/assets/transformer1.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBeSeWOEsM-J",
        "outputId": "d0684410-7ba6-4bd9-cc32-c05182119068"
      },
      "source": [
        "!pip install torchtext==0.6.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 18.2MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 14.9MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51kB 7.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 4.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.95 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCRTpPTfsUhC"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import ast\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "from torchtext.datasets import Multi30k\r\n",
        "from torchtext import data\r\n",
        "from torchtext.data import Field, BucketIterator\r\n",
        "\r\n",
        "import spacy\r\n",
        "\r\n",
        "import random\r\n",
        "import math\r\n",
        "import time\r\n",
        "\r\n",
        "SEED = 1234\r\n",
        "\r\n",
        "random.seed(SEED)\r\n",
        "np.random.seed(SEED)\r\n",
        "torch.manual_seed(SEED)\r\n",
        "torch.cuda.manual_seed(SEED)\r\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37KEfXZ6s11I",
        "outputId": "199f986f-e46d-428f-a681-43d6d759c27d"
      },
      "source": [
        "!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\r\n",
        "!unzip cornell_movie_dialogs_corpus.zip\r\n",
        "!ls /content/cornell\\ movie-dialogs\\ corpus"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-18 11:43:45--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9916637 (9.5M) [application/zip]\n",
            "Saving to: ‘cornell_movie_dialogs_corpus.zip’\n",
            "\n",
            "cornell_movie_dialo 100%[===================>]   9.46M  12.6MB/s    in 0.8s    \n",
            "\n",
            "2021-02-18 11:43:47 (12.6 MB/s) - ‘cornell_movie_dialogs_corpus.zip’ saved [9916637/9916637]\n",
            "\n",
            "Archive:  cornell_movie_dialogs_corpus.zip\n",
            "   creating: cornell movie-dialogs corpus/\n",
            "  inflating: cornell movie-dialogs corpus/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/cornell movie-dialogs corpus/\n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._.DS_Store  \n",
            "  inflating: cornell movie-dialogs corpus/chameleons.pdf  \n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._chameleons.pdf  \n",
            "  inflating: cornell movie-dialogs corpus/movie_characters_metadata.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_conversations.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_lines.txt  \n",
            "  inflating: cornell movie-dialogs corpus/movie_titles_metadata.txt  \n",
            "  inflating: cornell movie-dialogs corpus/raw_script_urls.txt  \n",
            "  inflating: cornell movie-dialogs corpus/README.txt  \n",
            "  inflating: __MACOSX/cornell movie-dialogs corpus/._README.txt  \n",
            "chameleons.pdf\t\t       movie_lines.txt\t\t  README.txt\n",
            "movie_characters_metadata.txt  movie_titles_metadata.txt\n",
            "movie_conversations.txt        raw_script_urls.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2I7zGu-tcUC"
      },
      "source": [
        "MAX_LENGTH = 20  # Maximum sentence length in Conversations\r\n",
        "\r\n",
        "cols=[\"lineID\",\"characterID\",\"movieID\",\"character name\",\"utterance\"]\r\n",
        "df_movie_lines=pd.read_csv(\"/content/cornell movie-dialogs corpus/movie_lines.txt\", sep='\\+\\+\\+\\$\\+\\+\\+',header=None,engine='python',names=cols)\r\n",
        "\r\n",
        "\r\n",
        "df_movie_lines.lineID = df_movie_lines.lineID.apply(lambda x: int(x[1:]))\r\n",
        "\r\n",
        "\r\n",
        "df_movie_lines = df_movie_lines.sort_values(by=['lineID'])\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akDSpik9zyOc"
      },
      "source": [
        "df_movie_lines.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybJuWJHz1xtN"
      },
      "source": [
        "dialogues = df_movie_lines.utterance.to_list()\r\n",
        "dialogues.pop(0)\r\n",
        "dialogues.append(np.nan)\r\n",
        "df_movie_lines['reply'] = dialogues"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "uyDhlTpT12_C",
        "outputId": "e5e5a1ab-d65c-4acc-f489-64aee0a67835"
      },
      "source": [
        "df_movie_lines.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lineID</th>\n",
              "      <th>characterID</th>\n",
              "      <th>movieID</th>\n",
              "      <th>character name</th>\n",
              "      <th>utterance</th>\n",
              "      <th>reply</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>49</td>\n",
              "      <td>u0</td>\n",
              "      <td>m0</td>\n",
              "      <td>BIANCA</td>\n",
              "      <td>Did you change your hair?</td>\n",
              "      <td>No.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>50</td>\n",
              "      <td>u3</td>\n",
              "      <td>m0</td>\n",
              "      <td>CHASTITY</td>\n",
              "      <td>No.</td>\n",
              "      <td>You might wanna think about it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>51</td>\n",
              "      <td>u0</td>\n",
              "      <td>m0</td>\n",
              "      <td>BIANCA</td>\n",
              "      <td>You might wanna think about it</td>\n",
              "      <td>I missed you.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>59</td>\n",
              "      <td>u9</td>\n",
              "      <td>m0</td>\n",
              "      <td>PATRICK</td>\n",
              "      <td>I missed you.</td>\n",
              "      <td>It says here you exposed yourself to a group ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>u8</td>\n",
              "      <td>m0</td>\n",
              "      <td>MISS PERKY</td>\n",
              "      <td>It says here you exposed yourself to a group ...</td>\n",
              "      <td>It was a bratwurst.  I was eating lunch.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   lineID  ...                                              reply\n",
              "0      49  ...                                                No.\n",
              "1      50  ...                     You might wanna think about it\n",
              "2      51  ...                                      I missed you.\n",
              "3      59  ...   It says here you exposed yourself to a group ...\n",
              "4      60  ...           It was a bratwurst.  I was eating lunch.\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eTZHxB_2CVK",
        "outputId": "df8eada1-4c02-4b66-8767-b40e3c370ddd"
      },
      "source": [
        "df_movie_lines.count()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lineID            304713\n",
              "characterID       304713\n",
              "movieID           304713\n",
              "character name    304713\n",
              "utterance         304446\n",
              "reply             304445\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-VKtNY32EdW"
      },
      "source": [
        "df_movie_lines['utterance'] = df_movie_lines['utterance'].astype('str')\r\n",
        "df_movie_lines['reply'] = df_movie_lines['reply'].astype('str')\r\n",
        "mask = ((df_movie_lines['utterance'].str.len() <= MAX_LENGTH) & (df_movie_lines['reply'].str.len() <= MAX_LENGTH))\r\n",
        "df_movie_lines = df_movie_lines.loc[mask]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln41cjVc2IKM"
      },
      "source": [
        "df_movie_lines = df_movie_lines.dropna()\r\n",
        "df_movie_lines.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCrNM3sm2KyT"
      },
      "source": [
        "Input = data.Field(tokenize = 'spacy',\r\n",
        "            init_token='<sos>', \r\n",
        "            eos_token='<eos>', \r\n",
        "            lower=True)\r\n",
        "\r\n",
        "Output = data.Field(tokenize = 'spacy',\r\n",
        "                    init_token='<sos>', \r\n",
        "                    eos_token='<eos>', \r\n",
        "                    lower=True)\r\n",
        "LineId = data.Field(sequential=False, use_vocab=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbM1Lw1z2Mr-"
      },
      "source": [
        "fields = [('Input', Input),('Output', Output), ('LineId', LineId)]\r\n",
        "example = [data.Example.fromlist([df_movie_lines.utterance[i],df_movie_lines.reply[i], df_movie_lines.lineID[i]], fields) for i in range(df_movie_lines.shape[0])]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X82Ow8Xz2Qff"
      },
      "source": [
        "N = len(example)\r\n",
        "train_len = 0.7*N\r\n",
        "\r\n",
        "train_data = data.Dataset(example[:int(train_len)], fields)\r\n",
        "valid_data = data.Dataset(example[int(train_len):], fields)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p41gHVgD2T_H"
      },
      "source": [
        "\r\n",
        "Input.build_vocab(train_data, min_freq = 3)\r\n",
        "Output.build_vocab(train_data, min_freq = 3)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZndXmj42Vl3",
        "outputId": "b7b2279b-5b12-43ca-86b2-98cae0b75d32"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "# device = torch.device('cpu')\r\n",
        "\r\n",
        "device"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCSFdtx02Z-b"
      },
      "source": [
        "\r\n",
        "BATCH_SIZE = 32\r\n",
        "\r\n",
        "# train_iterator, valid_iterator = Iterator.splits((train_data, valid_data), batch_size = BATCH_SIZE, \r\n",
        "#                                                             sort_key = lambda x: -x.LineId,\r\n",
        "#                                                             sort_within_batch=True, device = device)\r\n",
        "train_iterator = BucketIterator(train_data, batch_size=BATCH_SIZE, sort=True,\r\n",
        "                           sort_key = lambda x: -x.LineId,\r\n",
        "                           sort_within_batch=True, device = device)\r\n",
        "valid_iterator = BucketIterator(valid_data, batch_size=BATCH_SIZE, sort=True,\r\n",
        "                           sort_key = lambda x: -x.LineId,\r\n",
        "                           sort_within_batch=True, device = device)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TcSGsVy2cpk"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 input_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim,\r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 120):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \r\n",
        "                                                  n_heads, \r\n",
        "                                                  pf_dim,\r\n",
        "                                                  dropout, \r\n",
        "                                                  device) \r\n",
        "                                     for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        \r\n",
        "        batch_size = src.shape[0]\r\n",
        "        src_len = src.shape[1]\r\n",
        "        \r\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "        \r\n",
        "        #pos = [batch size, src len]\r\n",
        "\r\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            src = layer(src, src_mask)\r\n",
        "            \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "            \r\n",
        "        return src\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjudIT8P2hst"
      },
      "source": [
        "class EncoderLayer(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 hid_dim, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim,  \r\n",
        "                 dropout, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n",
        "                                                                     pf_dim, \r\n",
        "                                                                     dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        #src_mask = [batch size, 1, 1, src len] \r\n",
        "                \r\n",
        "        #self attention\r\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        #positionwise feedforward\r\n",
        "        _src = self.positionwise_feedforward(src)\r\n",
        "        \r\n",
        "        #dropout, residual and layer norm\r\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\r\n",
        "        \r\n",
        "        #src = [batch size, src len, hid dim]\r\n",
        "        \r\n",
        "        return src"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcDtv6iw2lNG"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        assert hid_dim % n_heads == 0\r\n",
        "        \r\n",
        "        self.hid_dim = hid_dim\r\n",
        "        self.n_heads = n_heads\r\n",
        "        self.head_dim = hid_dim // n_heads\r\n",
        "        \r\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, query, key, value, mask = None):\r\n",
        "        \r\n",
        "        batch_size = query.shape[0]\r\n",
        "        \r\n",
        "        #query = [batch size, query len, hid dim]\r\n",
        "        #key = [batch size, key len, hid dim]\r\n",
        "        #value = [batch size, value len, hid dim]\r\n",
        "                \r\n",
        "        Q = self.fc_q(query)\r\n",
        "        K = self.fc_k(key)\r\n",
        "        V = self.fc_v(value)\r\n",
        "        \r\n",
        "        #Q = [batch size, query len, hid dim]\r\n",
        "        #K = [batch size, key len, hid dim]\r\n",
        "        #V = [batch size, value len, hid dim]\r\n",
        "                \r\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\r\n",
        "        \r\n",
        "        #Q = [batch size, n heads, query len, head dim]\r\n",
        "        #K = [batch size, n heads, key len, head dim]\r\n",
        "        #V = [batch size, n heads, value len, head dim]\r\n",
        "                \r\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\r\n",
        "        \r\n",
        "        #energy = [batch size, n heads, query len, key len]\r\n",
        "        \r\n",
        "        if mask is not None:\r\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\r\n",
        "        \r\n",
        "        attention = torch.softmax(energy, dim = -1)\r\n",
        "                \r\n",
        "        #attention = [batch size, n heads, query len, key len]\r\n",
        "                \r\n",
        "        x = torch.matmul(self.dropout(attention), V)\r\n",
        "        \r\n",
        "        #x = [batch size, n heads, query len, head dim]\r\n",
        "        \r\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\r\n",
        "        \r\n",
        "        #x = [batch size, query len, n heads, head dim]\r\n",
        "        \r\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\r\n",
        "        \r\n",
        "        #x = [batch size, query len, hid dim]\r\n",
        "        \r\n",
        "        x = self.fc_o(x)\r\n",
        "        \r\n",
        "        #x = [batch size, query len, hid dim]\r\n",
        "        \r\n",
        "        return x, attention"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1mXt-wM2oum"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\r\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, hid dim]\r\n",
        "        \r\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, pf dim]\r\n",
        "        \r\n",
        "        x = self.fc_2(x)\r\n",
        "        \r\n",
        "        #x = [batch size, seq len, hid dim]\r\n",
        "        \r\n",
        "        return x"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhQKa_7m2qYA"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 output_dim, \r\n",
        "                 hid_dim, \r\n",
        "                 n_layers, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim, \r\n",
        "                 dropout, \r\n",
        "                 device,\r\n",
        "                 max_length = 120):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\r\n",
        "        \r\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \r\n",
        "                                                  n_heads, \r\n",
        "                                                  pf_dim, \r\n",
        "                                                  dropout, \r\n",
        "                                                  device)\r\n",
        "                                     for _ in range(n_layers)])\r\n",
        "        \r\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\r\n",
        "        \r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\r\n",
        "        \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "                \r\n",
        "        batch_size = trg.shape[0]\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "                            \r\n",
        "        #pos = [batch size, trg len]\r\n",
        "\r\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\r\n",
        "                \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        output = self.fc_out(trg)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "            \r\n",
        "        return output, attention"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs3G_CAO2tdC"
      },
      "source": [
        "class DecoderLayer(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 hid_dim, \r\n",
        "                 n_heads, \r\n",
        "                 pf_dim, \r\n",
        "                 dropout, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \r\n",
        "                                                                     pf_dim, \r\n",
        "                                                                     dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        \r\n",
        "        #self attention\r\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "            \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "            \r\n",
        "        #encoder attention\r\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\r\n",
        "        # query, key, value\r\n",
        "        \r\n",
        "        #dropout, residual connection and layer norm\r\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "                    \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        \r\n",
        "        #positionwise feedforward\r\n",
        "        _trg = self.positionwise_feedforward(trg)\r\n",
        "        \r\n",
        "        #dropout, residual and layer norm\r\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len, hid dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        return trg, attention"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9vVzJYS2xhh"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 encoder, \r\n",
        "                 decoder, \r\n",
        "                 src_pad_idx, \r\n",
        "                 trg_pad_idx, \r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "        \r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.src_pad_idx = src_pad_idx\r\n",
        "        self.trg_pad_idx = trg_pad_idx\r\n",
        "        self.device = device\r\n",
        "        \r\n",
        "    def make_src_mask(self, src):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        \r\n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "\r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "\r\n",
        "        return src_mask\r\n",
        "    \r\n",
        "    def make_trg_mask(self, trg):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        \r\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "        \r\n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\r\n",
        "        \r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\r\n",
        "        \r\n",
        "        #trg_sub_mask = [trg len, trg len]\r\n",
        "            \r\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\r\n",
        "        \r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        \r\n",
        "        return trg_mask\r\n",
        "\r\n",
        "    def forward(self, src, trg):\r\n",
        "        \r\n",
        "        #src = [batch size, src len]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "                \r\n",
        "        src_mask = self.make_src_mask(src)\r\n",
        "        trg_mask = self.make_trg_mask(trg)\r\n",
        "        \r\n",
        "        #src_mask = [batch size, 1, 1, src len]\r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        \r\n",
        "        enc_src = self.encoder(src, src_mask)\r\n",
        "        \r\n",
        "        #enc_src = [batch size, src len, hid dim]\r\n",
        "                \r\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\r\n",
        "        \r\n",
        "        #output = [batch size, trg len, output dim]\r\n",
        "        #attention = [batch size, n heads, trg len, src len]\r\n",
        "        \r\n",
        "        return output, attention"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Acb2kNrD22Rb"
      },
      "source": [
        "INPUT_DIM = len(Input.vocab)\r\n",
        "OUTPUT_DIM = len(Output.vocab)\r\n",
        "HID_DIM = 256\r\n",
        "ENC_LAYERS = 3\r\n",
        "DEC_LAYERS = 3\r\n",
        "ENC_HEADS = 8\r\n",
        "DEC_HEADS = 8\r\n",
        "ENC_PF_DIM = 512\r\n",
        "DEC_PF_DIM = 512\r\n",
        "ENC_DROPOUT = 0.1\r\n",
        "DEC_DROPOUT = 0.1\r\n",
        "\r\n",
        "enc = Encoder(INPUT_DIM, \r\n",
        "              HID_DIM, \r\n",
        "              ENC_LAYERS, \r\n",
        "              ENC_HEADS, \r\n",
        "              ENC_PF_DIM, \r\n",
        "              ENC_DROPOUT, \r\n",
        "              device)\r\n",
        "\r\n",
        "dec = Decoder(OUTPUT_DIM, \r\n",
        "              HID_DIM, \r\n",
        "              DEC_LAYERS, \r\n",
        "              DEC_HEADS, \r\n",
        "              DEC_PF_DIM, \r\n",
        "              DEC_DROPOUT, \r\n",
        "              device)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KppQLWWg237_"
      },
      "source": [
        "SRC_PAD_IDX = Input.vocab.stoi[Input.pad_token]\r\n",
        "TRG_PAD_IDX = Output.vocab.stoi[Output.pad_token]\r\n",
        "\r\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVqh38a2250W",
        "outputId": "d7ad4743-1931-48c4-e9fa-48a4ae4684a9"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 4,932,517 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuWTgQOq27DE"
      },
      "source": [
        "\r\n",
        "def initialize_weights(m):\r\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\r\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyQOODyo29OT"
      },
      "source": [
        "model.apply(initialize_weights);\r\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C48O9-LU3C9a"
      },
      "source": [
        "LEARNING_RATE = 0.0005\r\n",
        "\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSL5NsIz3EUK"
      },
      "source": [
        "def maskNLLLoss(inp, target, mask):\r\n",
        "    # print(inp.shape, target.shape, mask.sum())\r\n",
        "    nTotal = mask.sum()\r\n",
        "    crossEntropy = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\r\n",
        "    loss = crossEntropy(inp, target)\r\n",
        "    loss = loss.to(device)\r\n",
        "    return loss, nTotal.item()\r\n",
        "\r\n",
        "def original_maskNLLLoss(inp, target, mask):\r\n",
        "    nTotal = mask.sum()\r\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\r\n",
        "    loss = crossEntropy.masked_select(mask).mean()\r\n",
        "    loss = loss.to(device)\r\n",
        "    return loss, nTotal.item()"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m4HKCl53HLH"
      },
      "source": [
        "\r\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\r\n",
        "criterion = maskNLLLoss"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE7vBiG03Kff"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "def make_trg_mask(trg):\r\n",
        "        \r\n",
        "        #trg = [batch size, trg len]\r\n",
        "        \r\n",
        "        trg_pad_mask = (trg != TRG_PAD_IDX).unsqueeze(1).unsqueeze(2)\r\n",
        "        \r\n",
        "        #trg_pad_mask = [batch size, 1, 1, trg len]\r\n",
        "        \r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        \r\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = device)).bool()\r\n",
        "        \r\n",
        "        #trg_sub_mask = [trg len, trg len]\r\n",
        "            \r\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\r\n",
        "        \r\n",
        "        #trg_mask = [batch size, 1, trg len, trg len]\r\n",
        "        \r\n",
        "        return trg_mask\r\n",
        "\r\n",
        "def train(model, iterator, optimizer, criterion, clip):\r\n",
        "    \r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    n_totals = 0\r\n",
        "    print_losses = []\r\n",
        "    for i, batch in tqdm(enumerate(iterator), total=len(iterator)):\r\n",
        "        # print(batch)\r\n",
        "        loss = 0\r\n",
        "        src = batch.Input.permute(1, 0)\r\n",
        "        trg = batch.Output.permute(1, 0)\r\n",
        "        trg_mask = make_trg_mask(trg)\r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        output, _ = model(src, trg[:,:-1])\r\n",
        "                \r\n",
        "        #output = [batch size, trg len - 1, output dim]\r\n",
        "        #trg = [batch size, trg len]\r\n",
        "            \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "        output = output.contiguous().view(-1, output_dim)\r\n",
        "        trg = trg[:,1:].contiguous().view(-1)\r\n",
        "                \r\n",
        "        #output = [batch size * trg len - 1, output dim]\r\n",
        "        #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "        mask_loss, nTotal = criterion(output, trg, trg_mask)\r\n",
        "        \r\n",
        "        mask_loss.backward()\r\n",
        "        \r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        \r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        print_losses.append(mask_loss.item() * nTotal)\r\n",
        "        n_totals += nTotal\r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "    return sum(print_losses) / n_totals"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g173Eru3OBZ"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    n_totals = 0\r\n",
        "    print_losses = []\r\n",
        "    \r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for i, batch in tqdm(enumerate(iterator), total=len(iterator)):\r\n",
        "\r\n",
        "            src = batch.Input.permute(1, 0)\r\n",
        "            trg = batch.Output.permute(1, 0)\r\n",
        "            trg_mask = make_trg_mask(trg)\r\n",
        "\r\n",
        "            output, _ = model(src, trg[:,:-1])\r\n",
        "            \r\n",
        "            #output = [batch size, trg len - 1, output dim]\r\n",
        "            #trg = [batch size, trg len]\r\n",
        "            \r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            \r\n",
        "            output = output.contiguous().view(-1, output_dim)\r\n",
        "            trg = trg[:,1:].contiguous().view(-1)\r\n",
        "            \r\n",
        "            #output = [batch size * trg len - 1, output dim]\r\n",
        "            #trg = [batch size * trg len - 1]\r\n",
        "            \r\n",
        "            mask_loss, nTotal = criterion(output, trg, trg_mask)\r\n",
        "\r\n",
        "            print_losses.append(mask_loss.item() * nTotal)\r\n",
        "            n_totals += nTotal\r\n",
        "\r\n",
        "        \r\n",
        "    return sum(print_losses) / n_totals"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzl3yqI93Qmh"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxkGpsVG3Seg",
        "outputId": "8434c4e0-3484-4785-9ab1-910deaa32845"
      },
      "source": [
        "N_EPOCHS = 50\r\n",
        "CLIP = 1\r\n",
        "\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "    \r\n",
        "    start_time = time.time()\r\n",
        "    \r\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\r\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    end_time = time.time()\r\n",
        "    \r\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "    \r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\r\n",
        "    \r\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [01:56<00:00,  4.38it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.02it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 2m 9s\n",
            "\tTrain Loss: 2.765 | Train PPL:  15.880\n",
            "\t Val. Loss: 2.398 |  Val. PPL:  11.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [01:56<00:00,  4.37it/s]\n",
            "100%|██████████| 219/219 [00:13<00:00, 16.82it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Time: 2m 9s\n",
            "\tTrain Loss: 2.423 | Train PPL:  11.277\n",
            "\t Val. Loss: 2.297 |  Val. PPL:   9.948\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [01:57<00:00,  4.35it/s]\n",
            "100%|██████████| 219/219 [00:13<00:00, 16.68it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Time: 2m 10s\n",
            "\tTrain Loss: 2.303 | Train PPL:  10.006\n",
            "\t Val. Loss: 2.247 |  Val. PPL:   9.457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [01:57<00:00,  4.34it/s]\n",
            "100%|██████████| 219/219 [00:13<00:00, 16.81it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Time: 2m 10s\n",
            "\tTrain Loss: 2.218 | Train PPL:   9.189\n",
            "\t Val. Loss: 2.220 |  Val. PPL:   9.208\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [01:58<00:00,  4.31it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.02it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Time: 2m 11s\n",
            "\tTrain Loss: 2.155 | Train PPL:   8.630\n",
            "\t Val. Loss: 2.212 |  Val. PPL:   9.137\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [01:57<00:00,  4.33it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.21it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 06 | Time: 2m 10s\n",
            "\tTrain Loss: 2.097 | Train PPL:   8.138\n",
            "\t Val. Loss: 2.221 |  Val. PPL:   9.219\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [01:58<00:00,  4.32it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.31it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 07 | Time: 2m 10s\n",
            "\tTrain Loss: 2.054 | Train PPL:   7.798\n",
            "\t Val. Loss: 2.233 |  Val. PPL:   9.331\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [01:58<00:00,  4.29it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.12it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 08 | Time: 2m 11s\n",
            "\tTrain Loss: 2.016 | Train PPL:   7.507\n",
            "\t Val. Loss: 2.235 |  Val. PPL:   9.348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:01<00:00,  4.21it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.12it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 09 | Time: 2m 13s\n",
            "\tTrain Loss: 1.979 | Train PPL:   7.238\n",
            "\t Val. Loss: 2.240 |  Val. PPL:   9.391\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:01<00:00,  4.20it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.07it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10 | Time: 2m 14s\n",
            "\tTrain Loss: 1.947 | Train PPL:   7.006\n",
            "\t Val. Loss: 2.239 |  Val. PPL:   9.381\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:00<00:00,  4.23it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.30it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 11 | Time: 2m 13s\n",
            "\tTrain Loss: 1.916 | Train PPL:   6.794\n",
            "\t Val. Loss: 2.263 |  Val. PPL:   9.609\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:02<00:00,  4.18it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.02it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 12 | Time: 2m 14s\n",
            "\tTrain Loss: 1.888 | Train PPL:   6.604\n",
            "\t Val. Loss: 2.272 |  Val. PPL:   9.702\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:01<00:00,  4.18it/s]\n",
            "100%|██████████| 219/219 [00:13<00:00, 16.82it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 13 | Time: 2m 14s\n",
            "\tTrain Loss: 1.863 | Train PPL:   6.442\n",
            "\t Val. Loss: 2.280 |  Val. PPL:   9.780\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:02<00:00,  4.16it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.22it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 14 | Time: 2m 15s\n",
            "\tTrain Loss: 1.837 | Train PPL:   6.278\n",
            "\t Val. Loss: 2.288 |  Val. PPL:   9.859\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:02<00:00,  4.15it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.28it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 15 | Time: 2m 15s\n",
            "\tTrain Loss: 1.813 | Train PPL:   6.128\n",
            "\t Val. Loss: 2.303 |  Val. PPL:  10.005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:03<00:00,  4.12it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.51it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 16 | Time: 2m 16s\n",
            "\tTrain Loss: 1.794 | Train PPL:   6.015\n",
            "\t Val. Loss: 2.320 |  Val. PPL:  10.172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:06<00:00,  4.05it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.15it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 17 | Time: 2m 18s\n",
            "\tTrain Loss: 1.772 | Train PPL:   5.882\n",
            "\t Val. Loss: 2.327 |  Val. PPL:  10.248\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:07<00:00,  4.01it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.23it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 18 | Time: 2m 19s\n",
            "\tTrain Loss: 1.759 | Train PPL:   5.804\n",
            "\t Val. Loss: 2.337 |  Val. PPL:  10.350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:08<00:00,  3.96it/s]\n",
            "100%|██████████| 219/219 [00:13<00:00, 16.60it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 19 | Time: 2m 21s\n",
            "\tTrain Loss: 1.741 | Train PPL:   5.704\n",
            "\t Val. Loss: 2.340 |  Val. PPL:  10.377\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:09<00:00,  3.93it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.29it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 20 | Time: 2m 22s\n",
            "\tTrain Loss: 1.728 | Train PPL:   5.627\n",
            "\t Val. Loss: 2.341 |  Val. PPL:  10.396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:09<00:00,  3.94it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.26it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 21 | Time: 2m 22s\n",
            "\tTrain Loss: 1.713 | Train PPL:   5.547\n",
            "\t Val. Loss: 2.353 |  Val. PPL:  10.522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:11<00:00,  3.88it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.08it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 22 | Time: 2m 24s\n",
            "\tTrain Loss: 1.696 | Train PPL:   5.453\n",
            "\t Val. Loss: 2.369 |  Val. PPL:  10.685\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:10<00:00,  3.91it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.22it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 23 | Time: 2m 23s\n",
            "\tTrain Loss: 1.685 | Train PPL:   5.392\n",
            "\t Val. Loss: 2.377 |  Val. PPL:  10.769\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:12<00:00,  3.85it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.10it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 24 | Time: 2m 25s\n",
            "\tTrain Loss: 1.673 | Train PPL:   5.328\n",
            "\t Val. Loss: 2.385 |  Val. PPL:  10.862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:13<00:00,  3.83it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.18it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 25 | Time: 2m 25s\n",
            "\tTrain Loss: 1.661 | Train PPL:   5.266\n",
            "\t Val. Loss: 2.398 |  Val. PPL:  11.006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:12<00:00,  3.84it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.12it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 26 | Time: 2m 25s\n",
            "\tTrain Loss: 1.646 | Train PPL:   5.188\n",
            "\t Val. Loss: 2.417 |  Val. PPL:  11.210\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:14<00:00,  3.80it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 16.94it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 27 | Time: 2m 27s\n",
            "\tTrain Loss: 1.640 | Train PPL:   5.155\n",
            "\t Val. Loss: 2.428 |  Val. PPL:  11.335\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:15<00:00,  3.77it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.03it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 28 | Time: 2m 28s\n",
            "\tTrain Loss: 1.627 | Train PPL:   5.090\n",
            "\t Val. Loss: 2.427 |  Val. PPL:  11.327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:15<00:00,  3.77it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 16.85it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 29 | Time: 2m 28s\n",
            "\tTrain Loss: 1.618 | Train PPL:   5.044\n",
            "\t Val. Loss: 2.450 |  Val. PPL:  11.594\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:14<00:00,  3.78it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.19it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 30 | Time: 2m 27s\n",
            "\tTrain Loss: 1.608 | Train PPL:   4.993\n",
            "\t Val. Loss: 2.457 |  Val. PPL:  11.674\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:14<00:00,  3.80it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.15it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 31 | Time: 2m 26s\n",
            "\tTrain Loss: 1.599 | Train PPL:   4.949\n",
            "\t Val. Loss: 2.452 |  Val. PPL:  11.607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:15<00:00,  3.75it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.04it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 32 | Time: 2m 28s\n",
            "\tTrain Loss: 1.589 | Train PPL:   4.899\n",
            "\t Val. Loss: 2.465 |  Val. PPL:  11.764\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:17<00:00,  3.72it/s]\n",
            "100%|██████████| 219/219 [00:13<00:00, 16.73it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 33 | Time: 2m 30s\n",
            "\tTrain Loss: 1.577 | Train PPL:   4.842\n",
            "\t Val. Loss: 2.493 |  Val. PPL:  12.097\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:18<00:00,  3.69it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.02it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 34 | Time: 2m 31s\n",
            "\tTrain Loss: 1.574 | Train PPL:   4.827\n",
            "\t Val. Loss: 2.504 |  Val. PPL:  12.232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:17<00:00,  3.72it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.13it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 35 | Time: 2m 29s\n",
            "\tTrain Loss: 1.568 | Train PPL:   4.795\n",
            "\t Val. Loss: 2.506 |  Val. PPL:  12.251\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:18<00:00,  3.68it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 16.98it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 36 | Time: 2m 31s\n",
            "\tTrain Loss: 1.561 | Train PPL:   4.765\n",
            "\t Val. Loss: 2.509 |  Val. PPL:  12.296\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:19<00:00,  3.67it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 16.88it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 37 | Time: 2m 32s\n",
            "\tTrain Loss: 1.553 | Train PPL:   4.727\n",
            "\t Val. Loss: 2.527 |  Val. PPL:  12.510\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:19<00:00,  3.66it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.05it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 38 | Time: 2m 32s\n",
            "\tTrain Loss: 1.546 | Train PPL:   4.695\n",
            "\t Val. Loss: 2.533 |  Val. PPL:  12.595\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:19<00:00,  3.66it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 16.98it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 39 | Time: 2m 32s\n",
            "\tTrain Loss: 1.538 | Train PPL:   4.658\n",
            "\t Val. Loss: 2.532 |  Val. PPL:  12.579\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:20<00:00,  3.62it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 16.93it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 40 | Time: 2m 33s\n",
            "\tTrain Loss: 1.534 | Train PPL:   4.639\n",
            "\t Val. Loss: 2.545 |  Val. PPL:  12.748\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:22<00:00,  3.58it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.02it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 41 | Time: 2m 35s\n",
            "\tTrain Loss: 1.521 | Train PPL:   4.577\n",
            "\t Val. Loss: 2.571 |  Val. PPL:  13.085\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:22<00:00,  3.59it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.03it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 42 | Time: 2m 35s\n",
            "\tTrain Loss: 1.519 | Train PPL:   4.570\n",
            "\t Val. Loss: 2.562 |  Val. PPL:  12.959\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:22<00:00,  3.57it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.08it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 43 | Time: 2m 35s\n",
            "\tTrain Loss: 1.510 | Train PPL:   4.524\n",
            "\t Val. Loss: 2.598 |  Val. PPL:  13.436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:24<00:00,  3.53it/s]\n",
            "100%|██████████| 219/219 [00:13<00:00, 16.77it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 44 | Time: 2m 37s\n",
            "\tTrain Loss: 1.509 | Train PPL:   4.520\n",
            "\t Val. Loss: 2.586 |  Val. PPL:  13.279\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:24<00:00,  3.54it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 17.06it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 45 | Time: 2m 37s\n",
            "\tTrain Loss: 1.505 | Train PPL:   4.503\n",
            "\t Val. Loss: 2.602 |  Val. PPL:  13.494\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:24<00:00,  3.53it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 16.92it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 46 | Time: 2m 37s\n",
            "\tTrain Loss: 1.499 | Train PPL:   4.478\n",
            "\t Val. Loss: 2.612 |  Val. PPL:  13.627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:25<00:00,  3.51it/s]\n",
            "100%|██████████| 219/219 [00:13<00:00, 16.69it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 47 | Time: 2m 38s\n",
            "\tTrain Loss: 1.493 | Train PPL:   4.451\n",
            "\t Val. Loss: 2.617 |  Val. PPL:  13.693\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:26<00:00,  3.49it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 16.96it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 48 | Time: 2m 39s\n",
            "\tTrain Loss: 1.486 | Train PPL:   4.419\n",
            "\t Val. Loss: 2.635 |  Val. PPL:  13.939\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:25<00:00,  3.51it/s]\n",
            "100%|██████████| 219/219 [00:12<00:00, 16.91it/s]\n",
            "  0%|          | 0/510 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 49 | Time: 2m 38s\n",
            "\tTrain Loss: 1.478 | Train PPL:   4.382\n",
            "\t Val. Loss: 2.650 |  Val. PPL:  14.157\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 510/510 [02:26<00:00,  3.49it/s]\n",
            "100%|██████████| 219/219 [00:13<00:00, 16.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 50 | Time: 2m 39s\n",
            "\tTrain Loss: 1.478 | Train PPL:   4.383\n",
            "\t Val. Loss: 2.656 |  Val. PPL:  14.236\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D5NgvrL3VtR"
      },
      "source": [
        "import spacy\r\n",
        "spacy_en = spacy.load('en')\r\n",
        "\r\n",
        "def encode_inputs(input,vocab):\r\n",
        "\r\n",
        "  tokenized_input = [tok.text.lower() for tok in spacy_en.tokenizer(input)]\r\n",
        "  tokenized_input = ['<sos>'] + tokenized_input +['<eos>']\r\n",
        "\r\n",
        "  numericalized_input = [vocab[i] for i in tokenized_input]\r\n",
        "\r\n",
        "  tensor_input = torch.LongTensor([numericalized_input])\r\n",
        "  \r\n",
        "  return tensor_input\r\n",
        "\r\n",
        "def decode_outputs(output,vocab):\r\n",
        "  # output: [1,1,hid_dim]\r\n",
        "  predicted_token = output.argmax(-1)\r\n",
        "  return vocab[predicted_token.item()], predicted_token"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jczx-ZMv7GcJ",
        "outputId": "0da39367-f7b2-4d40-d781-168c443ac38c"
      },
      "source": [
        "print(\" Enter q or quit to exit.\")\r\n",
        "\r\n",
        "chatbot_answer_max_len = 20\r\n",
        "\r\n",
        "while(True):\r\n",
        "\r\n",
        "  input_ = input(\"Enter text:\")\r\n",
        "\r\n",
        "  if input_=='q' or input_=='quit':\r\n",
        "    break\r\n",
        "\r\n",
        "  src = encode_inputs(input_,Input.vocab.stoi).to(device)\r\n",
        "  src_mask = torch.ones([1,1,1,src.shape[-1]]).to(device)\r\n",
        "\r\n",
        "  trg = '<sos>'\r\n",
        "  trg = torch.LongTensor([Input.vocab.stoi[trg]]).unsqueeze(0).to(device)\r\n",
        "  trg_mask = torch.ones([1,1,1,1]).to(device)\r\n",
        "\r\n",
        "  with torch.no_grad():\r\n",
        "    enc_src = model.encoder(src,src_mask)\r\n",
        "  \r\n",
        "  decoder_outputs = []\r\n",
        "  for i in range(chatbot_answer_max_len):\r\n",
        "\r\n",
        "    with torch.no_grad():\r\n",
        "      decoder_output,_ = model.decoder(trg,enc_src,trg_mask,src_mask)\r\n",
        "\r\n",
        "    decoder_output_itos_value, trg = decode_outputs(decoder_output,Input.vocab.itos)\r\n",
        "\r\n",
        "    if decoder_output_itos_value == '<eos>':\r\n",
        "      break\r\n",
        "    decoder_outputs.append(decoder_output_itos_value)\r\n",
        "\r\n",
        "    \r\n",
        "  print(\" \".join(decoder_outputs))\r\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Enter q or quit to exit.\n",
            "Enter text:you are a good boy.\n",
            "  i can . after ?\n",
            "Enter text:were you not trained properly?\n",
            "  i <unk> . sure . sure . sure . sure . sure . sure . sure . sure .\n",
            "Enter text:you are not making any sense.\n",
            "  i can . after ?\n",
            "Enter text:will you?\n",
            "  i <unk> . sure . sure . sure . sure . sure . sure . sure . sure .\n",
            "Enter text:ok then, good buy for now\n",
            "  , ?\n",
            "Enter text:q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps5xvmRb7LQE"
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": []
    }
  ]
}